import streamlit as st
import json
import os
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ==========================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG
# ==========================================
st.set_page_config(page_title="Chatbot H·ªô Chi·∫øu Vi·ªát Nam", page_icon="üáªüá≥")

# L·∫•y API Key t·ª´ Secrets
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
else:
    st.error("‚ùå Ch∆∞a t√¨m th·∫•y API Key trong Secrets!")
    st.stop()

# ------------------------------------------
# T·ª∞ ƒê·ªòNG T√åM T√äN MODEL ƒê√öNG (S·ª¨A L·ªñI 404)
# ------------------------------------------
@st.cache_resource
def find_correct_model_name():
    try:
        # L·∫•y danh s√°ch t·∫•t c·∫£ model c√≥ h·ªó tr·ª£ generateContent
        available_models = [
            m.name for m in genai.list_models() 
            if 'generateContent' in m.supported_generation_methods
        ]
        # ∆Øu ti√™n t√¨m model Flash 1.5
        for name in available_models:
            if "1.5-flash" in name:
                return name
        # N·∫øu kh√¥ng th·∫•y Flash, th·ª≠ t√¨m b·∫£n Pro
        for name in available_models:
            if "pro" in name:
                return name
        return available_models[0]
    except Exception as e:
        # N·∫øu kh√¥ng li·ªát k√™ ƒë∆∞·ª£c, d√πng t√™n m·∫∑c ƒë·ªãnh ph·ªï bi·∫øn nh·∫•t
        return "models/gemini-1.5-flash"

AVAILABLE_MODEL = find_correct_model_name()

# ==========================================
# 2. X·ª¨ L√ù D·ªÆ LI·ªÜU (RAG)
# ==========================================
@st.cache_resource
def init_vector_db():
    if not os.path.exists("TAI_LIEU_RB.json"):
        return None
    
    client = chromadb.PersistentClient(path="chroma_db_data")
    # Model embedding nh·∫π cho Streamlit Cloud
    emb_func = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="paraphrase-multilingual-MiniLM-L12-v2"
    )
    
    try:
        collection = client.get_collection(name="RAG_passport", embedding_function=emb_func)
    except:
        collection = client.create_collection(name="RAG_passport", embedding_function=emb_func)
        with open("TAI_LIEU_RB.json", "r", encoding="utf-8") as f:
            data = json.load(f)
        collection.add(
            ids=[str(i) for i in range(len(data))],
            documents=[item["content_text"] for item in data],
            metadatas=[{"title": item["title"], "url": item["url"]} for item in data]
        )
    return collection

collection = init_vector_db()

# ==========================================
# 3. GIAO DI·ªÜN & CHAT
# ==========================================
st.title("üáªüá≥ Tr·ª£ l√Ω ·∫£o H·ªô chi·∫øu")
st.info(f"Ho·∫°t ƒë·ªông v·ªõi model: `{AVAILABLE_MODEL}`")

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

user_input = st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n...")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        with st.spinner("ƒêang tra c·ª©u..."):
            try:
                # T√¨m ki·∫øm trong database
                results = collection.query(query_texts=[user_input], n_results=2)
                context = "\n".join(results["documents"][0])
                
                # G·ªçi Gemini v·ªõi t√™n model ƒë√£ t√¨m th·∫•y
                model = genai.GenerativeModel(model_name=AVAILABLE_MODEL)
                prompt = f"Ng·ªØ c·∫£nh: {context}\n\nC√¢u h·ªèi: {user_input}"
                response = model.generate_content(prompt)
                
                st.markdown(response.text)
                st.session_state.messages.append({"role": "assistant", "content": response.text})
            except Exception as e:
                st.error(f"L·ªói: {str(e)}")
