import streamlit as st
import json
import os
import chromadb
from chromadb.utils import embedding_functions
import google.generativeai as genai

# ==========================================
# 1. C·∫§U H√åNH H·ªÜ TH·ªêNG
# ==========================================
st.set_page_config(page_title="H·ªó tr·ª£ H·ªô chi·∫øu VN", page_icon="üáªüá≥")

# Ki·ªÉm tra Key trong Secrets
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
else:
    st.error("‚ùå Thi·∫øu API Key trong Secrets!")
    st.stop()

# ==========================================
# 2. X·ª¨ L√ù D·ªÆ LI·ªÜU (RAG)
# ==========================================
@st.cache_resource
def init_db():
    if not os.path.exists("TAI_LIEU_RB.json"):
        return None
    client = chromadb.PersistentClient(path="chroma_db_data")
    emb_func = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="paraphrase-multilingual-MiniLM-L12-v2"
    )
    try:
        collection = client.get_collection(name="passport_rag_final", embedding_function=emb_func)
    except:
        collection = client.create_collection(name="passport_rag_final", embedding_function=emb_func)
        with open("TAI_LIEU_RB.json", "r", encoding="utf-8") as f:
            data = json.load(f)
        collection.add(
            ids=[str(i) for i in range(len(data))],
            documents=[item["content_text"] for item in data],
            metadatas=[{"title": item["title"]} for item in data]
        )
    return collection

collection = init_db()

# ==========================================
# 3. CHI·∫æN THU·∫¨T T·ª∞ ƒê·ªòNG TH·ª¨ MODEL (MODEL CYCLING)
# ==========================================
def generate_with_fallback(prompt):
    # B∆∞·ªõc 1: L·∫•y danh s√°ch th·ª±c t·∫ø c√°c model m√† KEY n√†y d√πng ƒë∆∞·ª£c
    try:
        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
    except:
        available_models = ["models/gemini-1.5-flash", "models/gemini-1.5-pro", "models/gemini-pro"]

    # B∆∞·ªõc 2: Th·ª≠ t·ª´ng model trong danh s√°ch
    errors = []
    for model_name in available_models:
        try:
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            return response.text, model_name
        except Exception as e:
            errors.append(f"{model_name}: {str(e)}")
            continue
            
    # N·∫øu t·∫•t c·∫£ ƒë·ªÅu th·∫•t b·∫°i
    st.error("T·∫•t c·∫£ c√°c model ƒë·ªÅu kh√¥ng ph·∫£n h·ªìi. Chi ti·∫øt l·ªói:")
    for err in errors: st.write(err)
    return None, None

# ==========================================
# 4. GIAO DI·ªÜN
# ==========================================
st.title("üáªüá≥ Tr·ª£ l√Ω H·ªô chi·∫øu Vi·ªát Nam")

if "messages" not in st.session_state:
    st.session_state.messages = []

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

user_input = st.chat_input("Nh·∫≠p c√¢u h·ªèi...")

if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        with st.spinner("ƒêang k·∫øt n·ªëi AI..."):
            # T√¨m ki·∫øm ng·ªØ c·∫£nh
            results = collection.query(query_texts=[user_input], n_results=1)
            context = results["documents"][0][0] if results["documents"] else ""
            
            full_prompt = f"D·ªØ li·ªáu: {context}\n\nC√¢u h·ªèi: {user_input}\nTr·∫£ l·ªùi ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát."
            
            # G·ªçi h√†m Fallback
            answer, success_model = generate_with_fallback(full_prompt)
            
            if answer:
                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})
                st.caption(f"‚úÖ ƒê√£ ch·∫°y th√†nh c√¥ng tr√™n: `{success_model}`")
